{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install the required packages**"
      ],
      "metadata": {
        "id": "RxlDyNt2dopR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch numpy matplotlib scikit-learn tqdm"
      ],
      "metadata": {
        "id": "qwohYc_-a_mk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "pDH6SJ5abYJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_swiss_roll"
      ],
      "metadata": {
        "id": "uWVbvGAXa_j3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Device Setup**"
      ],
      "metadata": {
        "id": "bRLBPaknbmh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "YeSkPun9bBeU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Generation**"
      ],
      "metadata": {
        "id": "9HyeHILlb0IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a batch of samples from the Swiss Roll dataset\n",
        "def sample_batch(batch_size):\n",
        "    \"\"\"\n",
        "    Generate a batch of samples from the Swiss Roll dataset.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Normalized 2D data samples.\n",
        "    \"\"\"\n",
        "    # Generate Swiss Roll dataset with noise\n",
        "    data, _ = make_swiss_roll(n_samples=batch_size, noise=0.25)\n",
        "    # Project to 2D (use only the 1st and 3rd columns)\n",
        "    data = data[:, [0, 2]]\n",
        "    # Normalize the data to improve training stability\n",
        "    data = data / 10.0\n",
        "    return data"
      ],
      "metadata": {
        "id": "BKRKW0K4bBbZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Model Definition**"
      ],
      "metadata": {
        "id": "g_woBfEXb_YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP Model Definition\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, N=40, data_dim=2, hidden_dim=128):\n",
        "        \"\"\"\n",
        "        Initialize the MLP model.\n",
        "\n",
        "        Args:\n",
        "            N (int): Number of layers in the network tail.\n",
        "            data_dim (int): Dimensionality of the input data.\n",
        "            hidden_dim (int): Number of neurons in hidden layers.\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        # Define the network head with two linear layers and ReLU activations\n",
        "        self.network_head = nn.Sequential(\n",
        "            nn.Linear(data_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Define the network tail with N layers, each having a linear layer and ReLU activation\n",
        "        self.network_tail = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, data_dim * 2)  # Output mean and log variance\n",
        "            ) for _ in range(N)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, t: int):\n",
        "        \"\"\"\n",
        "        Forward pass through the MLP model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input data tensor.\n",
        "            t (int): Time step index.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output of the network tail at time step t.\n",
        "        \"\"\"\n",
        "        h = self.network_head(x)  # Pass through the network head\n",
        "        return self.network_tail[t](h)  # Pass through the network tail for the given time step\n"
      ],
      "metadata": {
        "id": "LazYKp7EbBYx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diffusion Model Definition**"
      ],
      "metadata": {
        "id": "ob9DDKNGcQsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diffusion Model Definition\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, model: nn.Module, n_steps=40, device=device):\n",
        "        \"\"\"\n",
        "        Initialize the Diffusion Model.\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The neural network model used for denoising.\n",
        "            n_steps (int): Number of diffusion steps.\n",
        "            device (str): The device to run the model on ('cpu' or 'cuda').\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize diffusion parameters\n",
        "        betas = torch.sigmoid(torch.linspace(-18, 10, n_steps)) * (3e-1 - 1e-5) + 1e-5\n",
        "        self.beta = betas\n",
        "        self.alpha = 1. - betas\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.n_steps = n_steps\n",
        "        self.sigma2 = betas\n",
        "\n",
        "    def forward_process(self, x0, t):\n",
        "        \"\"\"\n",
        "        Perform the forward diffusion process.\n",
        "\n",
        "        Args:\n",
        "            x0 (torch.Tensor): Initial data tensor.\n",
        "            t (int): Time step.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (mu_posterior, sigma_posterior, xt)\n",
        "                - mu_posterior (torch.Tensor): Posterior mean.\n",
        "                - sigma_posterior (torch.Tensor): Posterior variance.\n",
        "                - xt (torch.Tensor): Data at time step t.\n",
        "        \"\"\"\n",
        "        t = t - 1  # Adjust indexing to start at 0\n",
        "        alpha_cum_forward = self.alpha_bar[t]\n",
        "        noise = torch.randn_like(x0)  # Add Gaussian noise\n",
        "        xt = x0 * torch.sqrt(alpha_cum_forward) + noise * torch.sqrt(1. - alpha_cum_forward)\n",
        "\n",
        "        # Compute posterior mean and variance\n",
        "        mu_posterior = (x0 * torch.sqrt(alpha_cum_forward) + xt * torch.sqrt(self.beta[t])) / (\n",
        "            torch.sqrt(self.alpha_bar[t] / self.alpha[t]) + 1. / torch.sqrt(self.alpha[t]))\n",
        "        sigma_posterior = torch.sqrt(1. / (1. / (1. - self.alpha_bar[t] / self.alpha[t]) + 1. / self.sigma2[t]))\n",
        "\n",
        "        return mu_posterior, sigma_posterior, xt\n",
        "\n",
        "    def reverse(self, xt, t):\n",
        "        \"\"\"\n",
        "        Perform the reverse diffusion process.\n",
        "\n",
        "        Args:\n",
        "            xt (torch.Tensor): Data tensor at time step t.\n",
        "            t (int): Time step.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (mu, sigma, samples)\n",
        "                - mu (torch.Tensor): Mean predicted by the model.\n",
        "                - sigma (torch.Tensor): Standard deviation predicted by the model.\n",
        "                - samples (torch.Tensor): Generated samples.\n",
        "        \"\"\"\n",
        "        t = t - 1  # Adjust indexing to start at 0\n",
        "        if t == 0:\n",
        "            return None, None, xt  # No reverse process at t=0\n",
        "        mu, h = self.model(xt, t).chunk(2, dim=1)  # Split the output into mean and log-variance\n",
        "        sigma = torch.exp(h * 0.5)  # Compute standard deviation from log-variance\n",
        "        return mu, sigma, mu + torch.randn_like(xt) * sigma  # Generate samples\n",
        "\n",
        "    def sample(self, size):\n",
        "        \"\"\"\n",
        "        Generate samples from the model.\n",
        "\n",
        "        Args:\n",
        "            size (int): Number of samples to generate.\n",
        "\n",
        "        Returns:\n",
        "            list of torch.Tensor: List of generated samples at each diffusion step.\n",
        "        \"\"\"\n",
        "        noise = torch.randn((size, 2), device=self.device)  # Start with noise\n",
        "        samples = [noise]\n",
        "        for t in range(self.n_steps, 0, -1):\n",
        "            _, _, x = self.reverse(samples[-1], t)  # Reverse the diffusion process\n",
        "            samples.append(x)\n",
        "        return samples"
      ],
      "metadata": {
        "id": "aaDZxa9_bBWH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Function**"
      ],
      "metadata": {
        "id": "_p8-nXXwcceB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train(model, optimizer, nb_epochs=150000, batch_size=64000):\n",
        "    \"\"\"\n",
        "    Train the diffusion model.\n",
        "\n",
        "    Args:\n",
        "        model (DiffusionModel): The diffusion model to train.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for the model parameters.\n",
        "        nb_epochs (int): Number of training epochs.\n",
        "        batch_size (int): Size of each data batch.\n",
        "    \"\"\"\n",
        "    for _ in tqdm(range(nb_epochs)):\n",
        "        x0 = torch.from_numpy(sample_batch(batch_size)).float().to(device)  # Generate a batch of data\n",
        "        t = np.random.randint(2, 41)  # Random time step\n",
        "        mu_posterior, sigma_posterior, xt = model.forward_process(x0, t)  # Forward process\n",
        "        mu, sigma, _ = model.reverse(xt, t)  # Reverse process\n",
        "\n",
        "        # Compute KL divergence loss\n",
        "        loss = (torch.log(sigma) - torch.log(sigma_posterior) +\n",
        "                (sigma_posterior ** 2 + (mu_posterior - mu) ** 2) / (2 * sigma ** 2) - 0.5).mean()\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update model parameters\n"
      ],
      "metadata": {
        "id": "1WBHy-M4bBTf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Function**"
      ],
      "metadata": {
        "id": "yxoFrhYwcvCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Function\n",
        "def plot(model):\n",
        "    \"\"\"\n",
        "    Plot the diffusion process and generated samples.\n",
        "\n",
        "    Args:\n",
        "        model (DiffusionModel): The trained diffusion model.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))  # Set up the figure\n",
        "    x0 = sample_batch(5000)  # Generate a batch of data\n",
        "    x0_tensor = torch.from_numpy(x0).float().to(device)\n",
        "    x20 = model.forward_process(x0_tensor, 20)[-1].cpu().numpy()  # Data at time step 20\n",
        "    x40 = model.forward_process(x0_tensor, 40)[-1].cpu().numpy()  # Data at time step 40\n",
        "    data = [x0, x20, x40]\n",
        "\n",
        "    # Plot the data at different diffusion steps\n",
        "    for i, (t, title) in enumerate(zip([0, 20, 39], [r'$t=0$', r'$t=\\frac{T}{2}$', r'$t=T$'])):\n",
        "        plt.subplot(2, 3, 1 + i)\n",
        "        plt.scatter(data[i][:, 0], data[i][:, 1], alpha=.1, s=1)\n",
        "        plt.xlim([-2, 2])\n",
        "        plt.ylim([-2, 2])\n",
        "        plt.gca().set_aspect('equal')\n",
        "        if i == 0: plt.ylabel(r'$q(\\mathbf{x}^{(0...T)})$', fontsize=17, rotation=0, labelpad=60)\n",
        "        plt.title(title, fontsize=17)\n",
        "\n",
        "    # Plot generated samples\n",
        "    samples = model.sample(5000)\n",
        "    for i, t in enumerate([0, 20, 40]):\n",
        "        plt.subplot(2, 3, 4 + i)\n",
        "        plt.scatter(samples[-(t+1)][:, 0].cpu().numpy(), samples[-(t+1)][:, 1].cpu().numpy(), alpha=.1, s=1, c='r')\n",
        "        plt.xlim([-2, 2])\n",
        "        plt.ylim([-2, 2])\n",
        "        plt.gca().set_aspect('equal')\n",
        "        if i == 0: plt.ylabel(r'$p(\\mathbf{x}^{(0...T)})$', fontsize=17, rotation=0, labelpad=60)\n",
        "\n",
        "    plt.savefig(f\"Imgs/diffusion_model.png\", bbox_inches='tight')  # Save the plot\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "tOC4vWMubBQp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start the Training Process**"
      ],
      "metadata": {
        "id": "hHsJlR_vc2vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training process\n",
        "model_mlp = MLP(hidden_dim=128).to(device)  # Initialize MLP model\n",
        "model = DiffusionModel(model_mlp)  # Initialize Diffusion Model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Set up optimizer\n",
        "\n",
        "train(model, optimizer)  # Train the model\n",
        "plot(model)  # Plot the results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgOTH4eTbBOS",
        "outputId": "f34f0c32-0408-4b58-b154-c7fed13d0bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|‚ñè         | 3624/150000 [01:05<38:13, 63.82it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZj2neuNbBLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMNUIsNFbBJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}